{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1b3398-8844-436c-b4b9-ba6f7607c9b6",
   "metadata": {},
   "source": [
    "### Data Cleaning & Preprocessing\n",
    "\n",
    "Now that everything’s merged — demographics, biometrics, medical history — it’s time to clean. Not the most exciting part, but one of the most important.\n",
    "\n",
    "Right away, I can see a few issues: some missing values we’ll need to handle carefully, a diagnosis_date column that’s likely still a string, and possibly a few strange entries in cost. Oh, and we should double-check for duplicate patient IDs just to be safe.\n",
    "\n",
    "Let’s take it from the top and clean as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb03b58-7f6a-42fb-9342-c6f511c1c79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33146 entries, 0 to 33145\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   patient_id                33146 non-null  object \n",
      " 1   diagnosis                 30000 non-null  object \n",
      " 2   diagnosis_date            30000 non-null  object \n",
      " 3   visit_type                30000 non-null  object \n",
      " 4   cost                      29850 non-null  float64\n",
      " 5   bmi                       23913 non-null  float64\n",
      " 6   blood_pressure_systolic   24155 non-null  float64\n",
      " 7   blood_pressure_diastolic  24155 non-null  float64\n",
      " 8   cholesterol_total         23968 non-null  float64\n",
      " 9   age                       32821 non-null  float64\n",
      " 10  gender                    32982 non-null  object \n",
      " 11  ethnicity                 33146 non-null  object \n",
      " 12  zip_code                  33146 non-null  int64  \n",
      "dtypes: float64(6), int64(1), object(6)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "## Load merged dataset \n",
    "merged_df = pd.read_csv(\"Merged_Healthcare_Data.csv\")\n",
    "\n",
    "# Just checking the shape and structure\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00783d2f-63d7-4a57-843d-4b6133356cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert 'diagnosis_date' to proper datetime\n",
    "# We'll need this to analyze disease trends over time later\n",
    "\n",
    "merged_df['diagnosis_date'] = pd.to_datetime(merged_df['diagnosis_date'], errors='coerce') ## Converts the values in the 'diagnosis_date' column from strings into proper datetime objects using pandas.to_datetime(); errors= 'coerce' tp replace the blank or misspelled value with NAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a509664b-8a37-4aea-b253-077fbb58f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values summary:\n",
      " bmi                         9233\n",
      "cholesterol_total           9178\n",
      "blood_pressure_systolic     8991\n",
      "blood_pressure_diastolic    8991\n",
      "cost                        3296\n",
      "diagnosis                   3146\n",
      "diagnosis_date              3146\n",
      "visit_type                  3146\n",
      "age                          325\n",
      "gender                       164\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Before we clean or drop anything, let’s take stock of what's missing.\n",
    "missing = merged_df.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "print(\"Missing values summary:\\n\", missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fa6b9-daaf-4666-ab9e-a53e95a776aa",
   "metadata": {},
   "source": [
    "### What We're Seeing in the Missing Values\n",
    "\n",
    "Most of the missing data is in biometrics — patients who haven’t had screenings.\n",
    "A second major block is medical records — patients who show biometric risk but haven’t been diagnosed yet. These are *exactly* who Loblaw might want to flag.\n",
    "A few gaps in age/gender — we’ll flag these, not drop them.\n",
    "So in short, we’re keeping everything, but tagging it carefully so we can filter it smartly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5be1e4-4ef3-49ab-9630-bd92d542a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4 We're not dropping these patients — they might still show signs of risk or cost\n",
    "merged_df['age_missing'] = merged_df['age'].isnull()\n",
    "merged_df['gender_missing'] = merged_df['gender'].isnull()\n",
    "merged_df['ethnicity_missing'] = merged_df['ethnicity'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f529e9cd-8204-4482-83bf-ad55094d99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 5: Normalize Gender (make sure 'male', 'MALE', ' Male ' all match)\n",
    "merged_df['gender'] = merged_df['gender'].str.strip().str.capitalize() ## .str.strip()-> remove any leading spaces from each value;.str.capitalize()-> converts the first letter to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47f91fb2-3127-456a-a752-5860a25d9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6:  Keep nulls they might be legitimate unbilled visits.\n",
    "# Remove cost = 0 or < 0, likely data entry or system error\n",
    "merged_df = merged_df[(merged_df['cost'].isnull()) | (merged_df['cost'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3c9971f-1e42-40bf-9273-e28be3cba991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found: 0\n"
     ]
    }
   ],
   "source": [
    "##Step 7: Check for and Drop Exact Duplicate Rows\n",
    "duplicates = merged_df.duplicated()\n",
    "print(\"Duplicate rows found:\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44e8f18d-3d94-4782-a138-4127aaef9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##step 8:Save Cleaned Dataset\n",
    "merged_df.to_csv(\"Cleaned_Merged_Data.csv\", na_rep='NaN', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e094c-a3ca-404d-9838-d0363278fe04",
   "metadata": {},
   "source": [
    "## Summary of the Data cleaning and preprocessing \n",
    "The goal here wasn’t to make everything perfect — just usable and meaningful.\n",
    "\n",
    "Not every missing value needed fixing. Some of them told a story: patients who never showed up for screening, or who’ve had biometric flags but never been officially diagnosed. Those might be the people we most want to pay attention to.\n",
    "\n",
    "So I didn’t over-clean. I flagged what’s missing, fixed what would break things later (like cost errors or duplicate rows), and left the rest alone, for now.\n",
    "\n",
    "This gives us a dataset that’s honest. Not flawless, but functional. And ready for the next step: actually learning something from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
